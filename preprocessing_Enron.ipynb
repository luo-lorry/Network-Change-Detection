{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106a8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69f6ff",
   "metadata": {},
   "source": [
    "## preprocessing Enron txt file with tuples representing the (time, sender, receiver) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a505e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.read_csv('datasets/execs.email.linesnum.txt', delimiter=' ', header=None, names=['time', 'from', 'to'])\n",
    "emails['time'] = pd.to_datetime(emails['time'],unit='s')\n",
    "emails = emails[(emails['time']>='1998-11-01') & (emails['time']<'2002-07-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "ed0716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['week'] = emails['time'].dt.year.astype(str).str.cat(emails['time'].dt.week.astype(str).str.zfill(2),sep='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "3b5493b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 1998-46 -----\n",
      "----- 1998-47 -----\n",
      "----- 1998-48 -----\n",
      "----- 1998-49 -----\n",
      "----- 1998-50 -----\n",
      "----- 1998-51 -----\n",
      "----- 1998-52 -----\n",
      "----- 1998-53 -----\n",
      "----- 1999-01 -----\n",
      "----- 1999-02 -----\n",
      "----- 1999-03 -----\n",
      "----- 1999-04 -----\n",
      "----- 1999-05 -----\n",
      "----- 1999-06 -----\n",
      "----- 1999-08 -----\n",
      "----- 1999-09 -----\n",
      "----- 1999-10 -----\n",
      "----- 1999-11 -----\n",
      "----- 1999-12 -----\n",
      "----- 1999-13 -----\n",
      "----- 1999-15 -----\n",
      "----- 1999-18 -----\n",
      "----- 1999-19 -----\n",
      "----- 1999-20 -----\n",
      "----- 1999-21 -----\n",
      "----- 1999-22 -----\n",
      "----- 1999-23 -----\n",
      "----- 1999-24 -----\n",
      "----- 1999-25 -----\n",
      "----- 1999-26 -----\n",
      "----- 1999-27 -----\n",
      "----- 1999-28 -----\n",
      "----- 1999-29 -----\n",
      "----- 1999-30 -----\n",
      "----- 1999-31 -----\n",
      "----- 1999-32 -----\n",
      "----- 1999-33 -----\n",
      "----- 1999-34 -----\n",
      "----- 1999-35 -----\n",
      "----- 1999-36 -----\n",
      "----- 1999-37 -----\n",
      "----- 1999-38 -----\n",
      "----- 1999-39 -----\n",
      "----- 1999-40 -----\n",
      "----- 1999-41 -----\n",
      "----- 1999-42 -----\n",
      "----- 1999-43 -----\n",
      "----- 1999-44 -----\n",
      "----- 1999-45 -----\n",
      "----- 1999-46 -----\n",
      "----- 1999-47 -----\n",
      "----- 1999-48 -----\n",
      "----- 1999-49 -----\n",
      "----- 1999-50 -----\n",
      "----- 1999-51 -----\n",
      "----- 1999-52 -----\n",
      "----- 2000-01 -----\n",
      "----- 2000-02 -----\n",
      "----- 2000-03 -----\n",
      "----- 2000-04 -----\n",
      "----- 2000-05 -----\n",
      "----- 2000-06 -----\n",
      "----- 2000-07 -----\n",
      "----- 2000-08 -----\n",
      "----- 2000-09 -----\n",
      "----- 2000-10 -----\n",
      "----- 2000-11 -----\n",
      "----- 2000-12 -----\n",
      "----- 2000-13 -----\n",
      "----- 2000-14 -----\n",
      "----- 2000-15 -----\n",
      "----- 2000-16 -----\n",
      "----- 2000-17 -----\n",
      "----- 2000-18 -----\n",
      "----- 2000-19 -----\n",
      "----- 2000-20 -----\n",
      "----- 2000-21 -----\n",
      "----- 2000-22 -----\n",
      "----- 2000-23 -----\n",
      "----- 2000-24 -----\n",
      "----- 2000-25 -----\n",
      "----- 2000-26 -----\n",
      "----- 2000-27 -----\n",
      "----- 2000-28 -----\n",
      "----- 2000-29 -----\n",
      "----- 2000-30 -----\n",
      "----- 2000-31 -----\n",
      "----- 2000-32 -----\n",
      "----- 2000-33 -----\n",
      "----- 2000-34 -----\n",
      "----- 2000-35 -----\n",
      "----- 2000-36 -----\n",
      "----- 2000-37 -----\n",
      "----- 2000-38 -----\n",
      "----- 2000-39 -----\n",
      "----- 2000-40 -----\n",
      "----- 2000-41 -----\n",
      "----- 2000-42 -----\n",
      "----- 2000-43 -----\n",
      "----- 2000-44 -----\n",
      "----- 2000-45 -----\n",
      "----- 2000-46 -----\n",
      "----- 2000-47 -----\n",
      "----- 2000-48 -----\n",
      "----- 2000-49 -----\n",
      "----- 2000-50 -----\n",
      "----- 2000-51 -----\n",
      "----- 2000-52 -----\n",
      "----- 2001-01 -----\n",
      "----- 2001-02 -----\n",
      "----- 2001-03 -----\n",
      "----- 2001-04 -----\n",
      "----- 2001-05 -----\n",
      "----- 2001-06 -----\n",
      "----- 2001-07 -----\n",
      "----- 2001-08 -----\n",
      "----- 2001-09 -----\n",
      "----- 2001-10 -----\n",
      "----- 2001-11 -----\n",
      "----- 2001-12 -----\n",
      "----- 2001-13 -----\n",
      "----- 2001-14 -----\n",
      "----- 2001-15 -----\n",
      "----- 2001-16 -----\n",
      "----- 2001-17 -----\n",
      "----- 2001-18 -----\n",
      "----- 2001-19 -----\n",
      "----- 2001-20 -----\n",
      "----- 2001-21 -----\n",
      "----- 2001-22 -----\n",
      "----- 2001-23 -----\n",
      "----- 2001-24 -----\n",
      "----- 2001-25 -----\n",
      "----- 2001-26 -----\n",
      "----- 2001-27 -----\n",
      "----- 2001-28 -----\n",
      "----- 2001-29 -----\n",
      "----- 2001-30 -----\n",
      "----- 2001-31 -----\n",
      "----- 2001-32 -----\n",
      "----- 2001-33 -----\n",
      "----- 2001-34 -----\n",
      "----- 2001-35 -----\n",
      "----- 2001-36 -----\n",
      "----- 2001-37 -----\n",
      "----- 2001-38 -----\n",
      "----- 2001-39 -----\n",
      "----- 2001-40 -----\n",
      "----- 2001-41 -----\n",
      "----- 2001-42 -----\n",
      "----- 2001-43 -----\n",
      "----- 2001-44 -----\n",
      "----- 2001-45 -----\n",
      "----- 2001-46 -----\n",
      "----- 2001-47 -----\n",
      "----- 2001-48 -----\n",
      "----- 2001-49 -----\n",
      "----- 2001-50 -----\n",
      "----- 2001-51 -----\n",
      "----- 2001-52 -----\n",
      "----- 2002-01 -----\n",
      "----- 2002-02 -----\n",
      "----- 2002-03 -----\n",
      "----- 2002-04 -----\n",
      "----- 2002-05 -----\n",
      "----- 2002-06 -----\n",
      "----- 2002-07 -----\n",
      "----- 2002-08 -----\n",
      "----- 2002-09 -----\n",
      "----- 2002-10 -----\n",
      "----- 2002-11 -----\n",
      "----- 2002-12 -----\n",
      "----- 2002-13 -----\n",
      "----- 2002-14 -----\n",
      "----- 2002-15 -----\n",
      "----- 2002-16 -----\n",
      "----- 2002-17 -----\n",
      "----- 2002-18 -----\n",
      "----- 2002-19 -----\n",
      "----- 2002-21 -----\n",
      "----- 2002-22 -----\n",
      "----- 2002-23 -----\n",
      "----- 2002-24 -----\n",
      "----- 2002-25 -----\n"
     ]
    }
   ],
   "source": [
    "unique_weeks = emails['week'].unique()\n",
    "enron_glist = [[] for _ in range(unique_weeks.shape[-1])]\n",
    "for idx, week in enumerate(unique_weeks):\n",
    "    print(f\"----- {week} -----\")\n",
    "    edges = list(map(tuple, emails[emails['week']==week][['from', 'to']].values))\n",
    "    g = nx.DiGraph((x, y, {'weight': v}) for (x, y), v in Counter(edges).items())\n",
    "    enron_glist[idx] = g\n",
    "save_object(enron_glist, 'datasets/enron_weekly.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce2c32",
   "metadata": {},
   "source": [
    "## preprocessing SFHH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pd.read_csv('datasets/SFHH-conf-sensor.edges', names=['person1', 'person2', 'time'])\n",
    "conf['8min'] = conf['time'] // 480\n",
    "unique_times = conf['8min'].unique()\n",
    "conf_glist = [[] for _ in range(unique_times.shape[-1])]\n",
    "for idx, time in enumerate(unique_times):\n",
    "    print(f\"----- {time} -----\")\n",
    "    edges = list(map(tuple, conf[conf['8min']==time][['person1', 'person2']].values))\n",
    "    g = nx.Graph((x, y, {'weight': v}) for (x, y), v in Counter(edges).items())\n",
    "    conf_glist[idx] = g\n",
    "save_object(conf_glist, 'datasets/conf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b998b40",
   "metadata": {},
   "source": [
    "## preprocessing stackoverflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow = pd.read_csv('datasets/sx-stackoverflow-a2q.txt', delimiter=' ', names=['from', 'to', 'time'])\n",
    "stackoverflow['time'] = pd.to_datetime(stackoverflow['time'], unit='s')\n",
    "stackoverflow = stackoverflow[(stackoverflow['time'] > '2015-12-08') & (stackoverflow['time'] <= '2015-12-13')]\n",
    "stackoverflow['30min'] = stackoverflow['time'].dt.day.astype(str) + '-' + stackoverflow['time'].dt.hour.astype(str).str.zfill(2) + '-' + stackoverflow['time'].dt.minute.div(30).astype(int).astype(str).str.zfill(1)\n",
    "unique_times = stackoverflow['30min'].unique()\n",
    "\n",
    "stackoverflow_glist = []\n",
    "for idx, time in enumerate(unique_times):\n",
    "    print(f\"----- {time} -----\")\n",
    "    edges = list(map(tuple, stackoverflow[stackoverflow['30min']==time][['from', 'to']].values))\n",
    "    g = nx.DiGraph((x, y, {'weight': v}) for (x, y), v in Counter(edges).items())\n",
    "    stackoverflow_glist.append(g)\n",
    "save_object(stackoverflow_glist, 'datasets/stackoverflow.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
